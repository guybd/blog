---
title: "Accelerating Qwen3-8B Agent on IntelÂ® Coreâ„¢ Ultra with Depth-Pruned Draft Models" 
thumbnail: #/blog/assets/optimum_intel/intel_thumbnail.png
authors:
- user: imargulis
  guest: true
  org: Intel
- user: ofirzaf
  guest: true
  org: Intel  
- user: sguskin
  guest: true
  org: Intel  
- user: guybd
  guest: true
  org: Intel
---
****

# Accelerating Qwen3-8B Agent on IntelÂ® Coreâ„¢ Ultra with Depth-Pruned Draft Models

TL;DR:

-   [Qwen3-8B](https://qwenlm.github.io/blog/qwen3/) is one of the most exciting recent releasesâ€”a model with  native agentic capabilities, making it a natural fit for the AIPC.

-   With [OpenVINO.GenAI](https://github.com/openvinotoolkit/openvino.genai), weâ€™ve been able to accelerate generation by ~1.3Ã— using speculative decoding with a lightweight Qwen3-0.6B draft.

-   By applying a simple pruning process to the draft, we pushed the speedup even further to ~1.4Ã—

-   We wrapped this up by showing how these improvements can be used to run a fast, local AI Agent with ðŸ¤—[smolagents](https://github.com/huggingface/smolagents)

## Qwen3
Qwen3-8B is part of the latest Qwen family, trained with explicit agentic behaviors. It supports tool invocation, multi-step reasoning, and long-context handlingâ€”capabilities that make it well-suited for complex agent workflows. When integrated with frameworks like Hugging Face SmolAgents, QwenAgent, or AutoGen, it enables a wide range of agentic applications built around tool use and reasoning.
The combination of optimized inference and built-in agentic intelligence makes Qwen3-8B a compelling foundation for next-gen AI agents.


## Accelerating Qwen3-8B on IntelÂ® Coreâ„¢ Ultra with Speculative Decoding

We started by benchmarking Qwen3-8B on an Intel Lunar Lake AI PC with OpenVINO acceleration, establishing our baseline for further optimization.

[Speculative decoding](https://arxiv.org/abs/2211.17192) is a method to speed up auto-regressive generation. It works by using a smaller, faster model as a draft to propose multiple tokens in a single forward pass, which are then validated by the larger target model in one forward pass. In our setup, [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) served as the target model while [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B) was used as the draft. This approach delivered an average of 1.3Ã— speedup over the baseline.

```python
from openvino_genai import LLMPipeline, draft_model

target_path = "/path/to/target/model"

draft_path = "/path/to/draft/model"

device = "GPU"

model = LLMPipeline(target_path, device, draft_model=draft_model(draft_path, device))

streamer = lambda x: print(x, end="", flush=True)

model.generate("What is speculative decoding and how does it improve inference speed?", max_new_tokens=100, reamer=streamer)
```

## Pushing Performance Further

 The speedup from SD depends on the mean number of generated tokens per forward step of the target, $\gamma$ the speculation window size, and the ratio between the target and draft models' latency $c$. A smaller, faster (though less accurate) draft can often deliver greater acceleration. This inspired us to shrink the draft model while still preserving its quality, i.e. $E(\\# generated\ tokens)$.

$$
Speedup = \frac{E(\\# generated\ tokens)}{\gamma c + 1}
$$

Our [recent work](https://huggingface.co/papers/2411.11055) shows that model depth (number of layers) is a major contributor to inference latency. Building on insights from [Gromov et al.](https://huggingface.co/papers/2403.17887) and [Siddiqui et al.](https://arxiv.org/abs/2407.16286), who showed that layer pruning can be done with minimal accuracy loss, we pruned 6 of 28 layers from the Qwen3-0.6B draft.
To recover quality of the pruned draft model, we further finetuned it using synthetic data generated by Qwen3-8B.
The data was produced by generating responses to 500k prompts from [BAAI/Infinity-Instruct dataset](https://huggingface.co/datasets/BAAI/Infinity-Instruct).

The resulting pruned draft model delivered \~1.4x speedup compared to the baseline --- an improvement over the \~1.3Ã— gain achieved with the original draft. This outcome aligns with theoretical expectations - reducing draft latency improves the speculative ratio, enabling faster and more efficient inference.

This demonstrates how pruning + speculative decoding can unlock faster and more efficient inferenceâ€”making local AI agents even more practical.

Check out the [notebook](https://github.com/guybd/openvino_notebooks/blob/latest/supplementary_materials/notebooks/qwen-3/qwen3.ipynb) and the Qwen3-0.6B depth-pruned [draft model](https://huggingface.co/OpenVINO/Qwen3-pruned-6L-from-0.6B-int8-ov) to reproduce the results step by step


## Integration to Smolagents

To showcase the real-world potential, we deployed our optimized setup with the ðŸ¤— Smolagents library. With this integration, developers can plug in Qwen3-8B (paired with our pruned draft) to build agents that call APIs and external tools, write and execute code, handle long-context reasoning and run efficiently on IntelÂ® Coreâ„¢ Ultra.
The benefits arenâ€™t limited to Hugging Faceâ€”this model pairing can also be used seamlessly with frameworks like AutoGen or QwenAgent, further strengthening the agentic ecosystem.

In our demo, we assigned the accelerated Qwen3-based agent a task:
 ðŸ‘‰ Summarize the key features of the Qwen3 model series and present them in a slide deck.
Hereâ€™s how it worked:
1.	The agent used a web search tool to gather up-to-date information.
2.	It then switched to the Python interpreter to generate slides with the python-pptx library.
This simple workflow highlights just a fraction of the possibilities unlocked when accelerated Qwen3 models meet frameworks like SmolAgentsâ€”bringing practical, efficient AI agents to life on AI PC. Try it [here](https://github.com/guybd/openvino_notebooks/blob/latest/supplementary_materials/notebooks/qwen-3/smolagents/qwen3_agent.ipynb).

<iframe width="100%" style="aspect-ratio: 16 / 9;"src="https://youtu.be/_ng5jXkN1Qc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

> [!NOTE]
> **Performance and legal notices**
>
> - Performance results are based on testing as of September 2025.
> - Performance varies by use, configuration, and other factors. Learn more on the Performance Index site.
> - No product or component can be absolutely secure.
> - Your costs and results may vary.
> - Intel technologies may require enabled hardware, software, or service activation.
> - Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.
> - Other names and brands may be claimed as the property of others.
