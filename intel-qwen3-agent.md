---
title: "BAccelerating Qwen3-8B Agents on IntelÂ® Coreâ„¢ Ultra with Speculative Decoding and Depth-Pruned Draft Models" 
thumbnail: #/blog/assets/optimum_intel/intel_thumbnail.png
authors:
- user: imargulis
  guest: true
  org: Intel
- user: ofirzaf
  guest: true
  org: Intel  
- user: sguskin
  guest: true
  org: Intel  
- user: guybd
  guest: true
  org: Intel
---
****

TL;DR:

-   [Qwen3-8B](https://qwenlm.github.io/blog/qwen3/) is a promising recently released model with built-in
    agentic capabilities making it a perfect fit for the AIPC.

-   With OpenVINO.GenAI we can speedup generation by \~1.3x with
    speculative decoding and Qwen3-0.6B draft

-   We show how through a simple pruning process of the draft we can
    increase the speedup further to \~1.4x

-   We wrap it up by showing how to utilize this speedup to run a fast
    local AI Agent with ðŸ¤—/smolagents

# Qwen3
Qwen3-8B is part of the latest Qwen family, trained with explicit agentic capabilities. It supports tool invocation, multi-step reasoning, and long context, making it well-suited for agent workflows. Integrated with frameworks such as Hugging Face SmolAgents, QwenAgent, or AutoGen, it enables a wide range of agentic applications involving tool calling and reasoning.

# Accelerating Qwen3-8B on Intel Lunar Lake with Speculative Decoding

We began by benchmarking Qwen3-8B on an Intel Lunar Lake AI-PC using OpenVINO, which served as the baseline reference for
optimization.

[Speculative decoding](https://arxiv.org/abs/2211.17192) is a method to accelerate auto-regressive generation presented in this paper (link).
The method leverages a fast yet less accurate model as a draft to speculate and validate several tokens in one forward pass. We applied speculative decoding (SD) with Qwen3-8B as the target model and Qwen3-0.6B as the draft model. This configuration achieved a \~1.3Ã— speedup.

```python
from openvino_genai import LLMPipeline, draft_model

target_path = "/path/to/target/model\"

draft_path = "/path/to/draft/model\"

device = "GPU"

model = LLMPipeline(target_path, device, draft_model=draft_model(draft_path, device))

streamer = lambda x: print(x, end=\'\', flush=True)

model.generate(\"What is speculative decoding and how does it improve inference speed?\", max_new_tokens=100, reamer=streamer)
```

# Pushing Performance Further**

The speedup from SD depends on the mean number of generated tokens per forward step of the target, $\gamma$ the speculation window size, and the ratio between the target and draft models' latency $c$. A faster yet less accurate draft model can yield greater overall acceleration. This motivated us to further reduce the size of the draft model while trying to preserve its quality, i.e. $E(\# generated\ tokens)$.

$$Speedup = \frac{E(\# generated\ tokens)}{\gamma c + 1}$$

Our [recent work](https://huggingface.co/papers/2411.11055) shows that model depth, the total number of layers, plays a crucial role in determining inference latency. Combined with insights from Gromov et al., who demonstrated that layer pruning is possible with minimal accuracy loss, we pruned 6 of 28 layers from the Qwen3-0.6B draft model.
To recover performance, we then finetuned the pruned draft using the synthetic data generated by Qwen3-8B.

The data was produced by generating responses to 500k prompts from [BAAI/Infinity-Instruct dataset](https://huggingface.co/datasets/BAAI/Infinity-Instruct).

The resulting pruned draft model delivered \~1.4x speedup and up to 2x bandwidth reduction compared to the baseline --- an improvement over the \~1.3Ã— gain achieved with the original draft. This outcome aligns with theoretical expectations: reducing draft latency improves the speculative ratio, enabling faster and more efficient inference.

Notebook link -
[[https://github.com/openvinotoolkit/openvino_notebooks/compare/latest\...guybd:openvino_notebooks:latest]{.underline}](https://github.com/openvinotoolkit/openvino_notebooks/compare/latest...guybd:openvino_notebooks:latest)

# Integration to Smolagents

Finally, we deployed our optimized setup with ðŸ¤—/smolagents library.
Developers can now plug in Qwen3-8B (with our pruned draft) to build
agents that Call APIs and tools, write & execute code, handle
long-context reasoning and Run efficiently on IntelÂ® Coreâ„¢ Ultra.

Beyond Hugging Face, the same model pairing can be deployed seamlessly
with frameworks like Autogen, or QwenAgent, strengthening the agentic
ecosystem.

In this example, we demonstrate how our accelerated Qwen3 model performs
as an agent. We assign it a task: summarize the key features of the
Qwen3 model series and then present the findings in a slide deck. To
accomplish this, the agent first uses the web search tool to gather
information, then switches to the Python interpreter to generate a
presentation with the python-pptx library. This workflow showcases just
a glimpse of the powerful potential unlocked when accelerated Qwen3 is
combined with Hugging Face's smolagents.

[[https://drive.google.com/file/d/171qz04BzoZHs7-6IUbI8c-Ey9QSiiXGT/view?usp=drive_link]{.underline}](https://drive.google.com/file/d/171qz04BzoZHs7-6IUbI8c-Ey9QSiiXGT/view?usp=drive_link)

[[https://drive.google.com/file/d/1F-JP_kMXgn0RsPi4yqfE9G94NTx_yuM7/view?usp=drive_link]{.underline}](https://drive.google.com/file/d/1F-JP_kMXgn0RsPi4yqfE9G94NTx_yuM7/view?usp=drive_link)

Footnotes

Performance results are based on testing as of September 2025.

Performance varies by use, configuration, and other factors. Learn more
on the Performance Index site.

No product or component can be absolutely secure.

Your costs and results may vary.

Intel technologies may require enabled hardware, software, or service
activation.

Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are
trademarks of Intel Corporation or its subsidiaries.

Other names and brands may be claimed as the property of others.
